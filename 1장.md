## 머신러닝

- 명시적인 프로그래밍 없이 데이터를 이용해서 컴퓨터가 어떤 지식이나 패턴을 학습하는 것

어떤 문제 T, 관련된 경험 E, 성과 측정지표 P → 학습(Learn)

- 문제 T에 대한 성과는 P로 측정되고 경험 E로부터 개선을 진행한다.

T: 이메일이 스팸인지 아닌지 분류

E: 스팸필터가 레이블링한 이메일이 스팸인지 아닌지 관찰

P: 이메일이 스팸인지 아닌지 정확히 분류한 개수 혹은 비율

### Supervised Learning

- 정답 데이터가 존재하는 상황에서 학습하는 알고리즘
- 입력데이터 x와 그에 대한 정답 레이블 y의 쌍(x,y)를 이용해 학습하는 알고리즘
- 분류(Classification) 문제
    - 예측하는 결과 값이 이산값인 문제
- 회귀(Regression) 문제
    - 예측하는 결과값이 연속값인

### Unsupervised Learning

- 정답 레이블 y없이 입력 데이터 x만을 이용해 학습하는 알고리즘
- 입력 데이터 (x)형태로 학습을 진행
- **데이터의 숨겨진 특징을 찾아내는 것이 목적**
- 클러스터링 알고리즘
- 단독으로 사용하기보단 비지도 학습으로 파악한 데이터의 숨겨진 특징을 원본 데이터 대신 **지도학습의 인풋 데이터로 활용해서 지도 학습의 성능을 더욱 끌어올리는 용도로 많이 활용**
- 주성분 분석, 오토인코더

### Reinforcement Learning

- 앞의 두 알고리즘은 데이터가 이미 주어진 정적인 상태에서 학습
- 에이전트가 주어진 환경에서 어떤 행동을 취하고 이에 대한 보상을 얻으면서 학습을 진행
- 알파고
- **보상을 최대화 하도록 학습 진행**
- 동적인 상태에서 데이터를 수집하는 과정까지 학습 과정에 포함되어 있는 알고리즘

---

## 딥러닝

- 은닉층을 깊게 쌓은 구조를 이용해 학습
- 데이터의 특징을 단계별로 추상화를 높여가면서 학습
- 사람과 같은 고차원적 인지 활동을 수행할 수 있음
- 표현학습이라고도 불림
- Hand-Crafted Feature
- Learned Feature

### 잘 동작 문제영역

- 이미지, 자연어 , 음성 등 비정형화된 대량의 데이터로부터 **인식**을 수행하는 문제 영역에 잘 동작

### 잘 동작 하지 않는

- 데이터가 부족, 정형화된 데이터

딥러닝 알고리즘을 가능하게 만든 3가지 환경적 요인

- 빅데이터
- GPU
- 새로운 알고리즘의 등장

---

## 딥러닝, 텐서플로 응용분야

주요 응용 분야 

- 컴퓨터비전
- NLP
    - Google Duplex
- Speech Recognition 음성인식
    - Speech to text
    - 가상비서, 시리 ..., 인공지능 스피커
- Game
    - 인공지능의 성능을 측정하기 위해 사용
    - 알파고, OpenAI Five
    - 대표적인 머신러닝 알고리즘은 강화학습
- Gernerative Model
    - 학습데이터의 분포를 학습해 학습한 분포로부터 새로운 데이터 생성
    - 데이터 증대
    - Deepfake

### 딥러닝의 역사

인공신경망 아이디어 → 퍼셉트론 모형 → 선형 분리분가능 ⇒ 다른 방법 탐구→ Multi-Layer Perceptron, Backpropagation Algorithm ⇒ XOR 문제 해결(선형 분리 가능) → CNN 구조 → Vanishing Gradient Problem, SVM탐구 → Pre-training 을 이용, Deep Autoencoders → 빅데이터, GPU → AlexNet → DeepLearning+Reinforcement= Depp-Q-Networks(DQN) →GAN

---

## 머신러닝의 기본 프로세스

1. 학습하고자 하는 가설을 수학적 표현식으로 나타냄
2. 가설의 성능을 측정할 수 있는 손실함수 정의
3. 손실함수를 최소화 할 수 있는 알고리즘 설계

1. 가설 정의

y = Wx + b (x 인풋데이터, y 타겟 데이터, W,b 파라미터 theta, 트레이닝 데이터로부터 학습을 통해 적절한 값을 찾아야함)

1. 손실함수 정의
- 적절한 파라미터 값을 알아내기 위해 손실함수 정의
    - 평균제곱오차 (Mean of Squared Error(MSE))
- 풀고자 하는 목적에 가까운 형태로 최적화되면 더 작은 값을 가짐 ⇒ 비용함수(Cost function)
1. 최적화 정의 
- 손실 함수를 최소화 하는 방향으로 파라미터들을 업데이트 할 수 있는 학습 알고리즘 설계해야함
- 맨 처음에 랜덤한 값으로 파라미터 초기화, 후에 파라미터를 적절한 값으로 계속해서 업데이트
- 파라미터를 적절한 값으로 업데이트 하는 알고리즘 → **최적화기법**
    - 대표적으로 **경사하강법**
    - 현재 스텝의 파라미터에서 손실함수의 미분값에 러닝레이트를 곱한만큼을 빼서 다음 스템의 파라미터 값으로 지정
    - 논문과 대칭 시켜 유사한 learning rate 사용, 1/2씩 줄이기
    
    ### Batch Gradient Descent
    
    - 경사하강법의 한 스텝 업데이트 할 때 전체 트레이닝 데이터를 하나의 Batch로 만들어 사용함
    - 트레이닝 개수가 매우 많아지면 오랜 시간이 걸림
    
    ### Stochastic Gradient Descent
    
    - 한 스텝 업데이트를 진행할 때 1개의 트레이닝 데이터만 사용
    - 파라미터를 자주 업데이트
    - 각각 트레이닝 데이터의 특성만을 고려하므로 부정확한 방향으로 업데이트가 진행될 수 있음
    
    ### Mini-Batch Gradient Descent
    
    - 전체 트레이닝 데이터 Batch가 1000개라면 이를 100개씩 묶은 Mini-Batch

## Training Data, Validation Data, Test data

- 모델이 잘 학습됐는지 체크할 때 - 테스트, 실제 문제를 푸는 과정 - 추론

Overfitting, Underfitting

- 트레이닝 에러는 작아지지만 검증 에러는 커지는 지점에서 업데이트를 중지하면 최적의 파라미터를 얻을 수 있음

## 소프트맥스 회귀

- Normalization 함수, 출력값들의 합을 1로 만들어줌
- 확률론적 표현, 직관적
    
    ### 크로스 엔트로피 손실 함수
    
    - MSE와 같이 모델의 예측값이 참값과 비슷하면 작은 값, 참값과 다르면 큰 값을 갖는 형태의 함수
    - 일반적으로 분류문제에 대해서는 MSE보다 크로스엔트로피 함수를 사용하는 것이 학습이 더 잘됨

### MNIST 데이터셋

- 머신러닝

### On-hot Encoding

- 범주형 값을 이진화된 값으로 바꿔서 표현
- 해당 레이블을 나타내는 인덱스만 1의 값을 가지고 나머지 부분은 0의 값을 가진 binary value로 표현

---

## 다양한 컴퓨터비전 문제영역

---

### TensorFlow 2.0

- 구글에서 개발한 딥러닝/머신러닝을 위한 오픈소스 라이브러리
- 손쉬운 딥러닝 모델을 구현하게 하는 Python API 제공
- Mobile Device부터 멀티 GPU클러스터까지 지원하는 폭넓은 Portability
- 강력한 시각화를 지원하는 TensorBoard 제공
- 전세계적으로 폭넓은 사용자 Community
- 구글의 강력한 지원과 발빠른 신기능 업데이트

Tensor - n차원 행렬을 지칭

---

### **TensorFlow 2.0을 이용한 선형 회귀(Linear Regression) 알고리즘 구현**

```java
# -*- coding: utf-8 -*-

import tensorflow as tf

# 선형회귀 모델(Wx + b)을 위한 tf.Variable을 선언합니다.
W = tf.Variable(tf.random.normal(shape=[1]))
b = tf.Variable(tf.random.normal(shape=[1]))

# 1. 가설정의
@tf.function
def linear_model(x):
  return W*x + b

# 2. 손실 함수를 정의합니다.
# MSE 손실함수 \mean{(y' - y)^2}
@tf.function
def mse_loss(y_pred, y):
  return tf.reduce_mean(tf.square(y_pred - y))

# 3. 최적화를 위한 그라디언트 디센트 옵티마이저를 정의합니다.
# learning rate 0.01
optimizer = tf.optimizers.SGD(0.01)

# 최적화를 위한 function을 정의합니다.
@tf.function
def train_step(x, y):
  with tf.GradientTape() as tape:
    y_pred = linear_model(x)
    loss = mse_loss(y_pred, y)
#loss function 에 대한 해당 파라미터의 gradient 값 구함
  gradients = tape.gradient(loss, [W, b])
#계산한 gradient와 갱신 대상으로 사용할 파라미터들 묶어줌
  optimizer.apply_gradients(zip(gradients, [W, b]))

# 트레이닝을 위한 입력값과 출력값을 준비합니다. (y=2x)
x_train = [1, 2, 3, 4]
y_train = [2, 4, 6, 8]

# 경사하강법을 1000번 수행합니다.
for i in range(1000):
  train_step(x_train, y_train)

# 테스트를 위한 입력값을 준비합니다.
x_test = [3.5, 5, 5.5, 6]
# 테스트 데이터를 이용해 학습된 선형회귀 모델이 데이터의 경향성(y=2x)을 잘 학습했는지 측정합니다.
# 예상되는 참값 : [7, 10, 11, 12]
print(linear_model(x_test).numpy())
```

### TensorFlow 2.0 케라스 서브클래싱

- beginner style
    - sequential, compile, fit 이용 → 간결한 구현
    - 디테일한 부분 컨트롤 불편
- **expert style**
    - 디테일한 부분 컨트롤 가능
    - 복잡한 코드 구조, 모든 부분 직접 작성

---

### **TensorFlow 2.0과 Softmax Regression을 이용한 MNIST 숫자분류기 구현**

```java
# -*- coding: utf-8 -*-

import tensorflow as tf

# MNIST 데이터를 다운로드 합니다.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
# 이미지들을 int -> float32 데이터 타입으로 변경합니다.
x_train, x_test = x_train.astype('float32'), x_test.astype('float32')
# 28*28 형태의 이미지를 784차원(1,784)으로 flattening(펼치는것) 합니다.
# -1 앞에 있는 차원을 알아서 맞춰주는 magic number (60000)
x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])
# [0, 255] 사이의 값을 [0, 1]사이의 값으로 Normalize합니다. 오버피팅 막기위해?
x_train, x_test = x_train / 255., x_test / 255.
# 레이블 데이터에 one-hot encoding을 적용합니다.(60000 * 10)
y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)

# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.
# mini batch 100개씩 묶음 iter 순차적 가져옴
train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_data = train_data.repeat().shuffle(60000).batch(100)
train_data_iter = iter(train_data)

# ----- 1. 가설 정의 -----
# tf.keras.Model을 이용해서 Softmax Regression 모델을 정의합니다.
class SoftmaxRegression(tf.keras.Model): #인자 부분 상속받음
  def __init__(self): #생성자
    super(SoftmaxRegression, self).__init__()
		# W*x+b
    self.softmax_layer = tf.keras.layers.Dense(10, # output demension
                                               activation=None,
                                               kernel_initializer='zeros', # W
                                               bias_initializer='zeros') # b

  def call(self, x):
    logits = self.softmax_layer(x)

    return tf.nn.softmax(logits)

# ----- 2. 손실함수 정의 -----
# cross-entropy 손실 함수를 정의합니다.
@tf.function
def cross_entropy_loss(y_pred, y):
  return tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(y_pred), axis=[1]))
  #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logtis, labels=y)) 
	# tf.nn.softmax_cross_entropy_with_logits API를 이용한 구현, 위에서 softmax 씌우지 않고 logits 넘겨야함

# ----- 3. 옵티마이저 정의 -----
# 최적화를 위한 그라디언트 디센트 옵티마이저를 정의합니다.
optimizer = tf.optimizers.SGD(0.5) #learning rate 0.5

# 최적화를 위한 function을 정의합니다.
# 한 번의 gradient descent 수행
@tf.function
def train_step(model, x, y):
  with tf.GradientTape() as tape:
    y_pred = model(x)
    loss = cross_entropy_loss(y_pred, y)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

# 모델의 정확도를 출력하는 함수를 정의합니다.
@tf.function
def compute_accuracy(y_pred, y):
	# 정답과 일치하는 것들 판단, 10차원행렬에 모든 값이 들어있으므로 최댓값 찾음
  correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

  return accuracy

# SoftmaxRegression 모델을 선언합니다.
SoftmaxRegression_model = SoftmaxRegression()

# 1000번 반복을 수행하면서 최적화를 수행합니다.
for i in range(1000):
	# batch_xs (100, 784), batch_ys (100,10)
  batch_xs, batch_ys = next(train_data_iter)
  train_step(SoftmaxRegression_model, batch_xs, batch_ys)

# 학습이 끝나면 학습된 모델의 정확도를 출력합니다.
print("정확도(Accuracy): %f" % compute_accuracy(SoftmaxRegression_model(x_test), y_test)) # 정확도 : 약 91%
```

---

## **다층 퍼셉트론 MLP**

### 인공신경망

- 생물학적 신경망에서 아이디어를 얻음
- 컴퓨터는 순차 처리 연산기
- 인간의 뇌처럼 대량의 병렬 처리 연산하면 인지행동 가능?

### 퍼셉트론

- 생물학적 뉴런을 공학적인 구조로 변형한 그림
- y = a(W*x +b)  (a는 activation function 활성화 함수)
- 입력층, 출력층
- 입력값을 받으면 2개의 출력값(0, 1) 중 하나를 출력해내는 선형 이진분류기
- 가중치에 기반한 의사결정
- 단순한 선형분류기에 불과, 간단한 XOR 문제(선형 분리가 불가능한 문제) 해결할 수 없음

### 다층 퍼셉트론(Multi Layer Perceptron) = 인공신경망(ANN)

- 퍼셉트론을 여러층 쌓아 올린 다층 퍼셉트론 구조
- 선형 분리가 불가능한 문제도 해결할 수 있음
- 입력층, **은닉층**, 출력층
- 분류기가 비선형적인 특징을 학습할 수 있도록 함
- activation - sigmoid, tanh, ReLU 사용
- Hidden layer를 여러 번 쌓아 올린 형태 - 깊은 인공신경망(DNN)

---

## TensorFlow 2.0과 ANN을 이용한 MNIST 숫자분류기 구현

```java
# -*- coding: utf-8 -*-
# 텐서플로우를 이용한 ANN(Artificial Neural Networks) 구현 - Keras API를 이용한 구현

import tensorflow as tf

# MNIST 데이터를 다운로드 합니다.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
# 이미지들을 float32 데이터 타입으로 변경합니다.
x_train, x_test = x_train.astype('float32'), x_test.astype('float32')
# 28*28 형태의 이미지를 784차원으로 flattening 합니다.
x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])
# [0, 255] 사이의 값을 [0, 1]사이의 값으로 Normalize합니다.
x_train, x_test = x_train / 255., x_test / 255.
# 레이블 데이터에 one-hot encoding을 적용합니다.
y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)

# 학습을 위한 설정값들을 정의합니다.
learning_rate = 0.001 # gradient descent에서 알파 값
num_epochs = 30     # 학습횟수 30번 보겠다
batch_size = 256    # 배치개수
display_step = 1    # 손실함수 출력 주기
input_size = 784    # 28 * 28
hidden1_size = 256
hidden2_size = 256
output_size = 10

# tf.data API를 이용해서 데이터를 섞고(한번 epoch 끝날때마다) batch 형태로 가져옵니다.
train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_data = train_data.shuffle(60000).batch(batch_size)

#W, b값 
def random_normal_intializer_with_stddev_1(): 
  return tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None)

# tf.keras.Model을 이용해서 ANN 모델을 정의합니다.
class ANN(tf.keras.Model):
  def __init__(self):
    super(ANN, self).__init__()
    self.hidden_layer_1 = tf.keras.layers.Dense(hidden1_size,
                                                activation='relu',
                                                kernel_initializer=random_normal_intializer_with_stddev_1(),
                                                bias_initializer=random_normal_intializer_with_stddev_1())
    self.hidden_layer_2 = tf.keras.layers.Dense(hidden2_size,
                                                activation='relu',
                                                kernel_initializer=random_normal_intializer_with_stddev_1(),
                                                bias_initializer=random_normal_intializer_with_stddev_1())
    self.output_layer = tf.keras.layers.Dense(output_size,
                                              activation=None,
                                              kernel_initializer=random_normal_intializer_with_stddev_1(),
                                              bias_initializer=random_normal_intializer_with_stddev_1())

  def call(self, x):
    H1_output = self.hidden_layer_1(x)
    H2_output = self.hidden_layer_2(H1_output)
    logits = self.output_layer(H2_output)

    return logits

# cross-entropy 손실 함수를 정의합니다.
@tf.function
def cross_entropy_loss(logits, y):
  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))

# 최적화를 위한 Adam 옵티마이저를 정의합니다.
optimizer = tf.optimizers.Adam(learning_rate)

# 최적화를 위한 function을 정의합니다.
@tf.function
def train_step(model, x, y):
  with tf.GradientTape() as tape:
    y_pred = model(x)
    loss = cross_entropy_loss(y_pred, y)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

# 모델의 정확도를 출력하는 함수를 정의합니다.
@tf.function
def compute_accuracy(y_pred, y):
  correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

  return accuracy

# ANN 모델을 선언합니다.
ANN_model = ANN()

# 지정된 횟수만큼 최적화를 수행합니다.
for epoch in range(num_epochs):
  average_loss = 0.
  total_batch = int(x_train.shape[0] / batch_size)
  # 모든 배치들에 대해서 최적화를 수행합니다.
  for batch_x, batch_y in train_data:
    # 옵티마이저를 실행해서 파라마터들을 업데이트합니다.
    _, current_loss = train_step(ANN_model, batch_x, batch_y), cross_entropy_loss(ANN_model(batch_x), batch_y)
    # 평균 손실을 측정합니다.
    average_loss += current_loss / total_batch
  # 지정된 epoch마다 학습결과를 출력합니다.
  if epoch % display_step == 0:
    print("반복(Epoch): %d, 손실 함수(Loss): %f" % ((epoch+1), average_loss))

# 테스트 데이터를 이용해서 학습된 모델이 얼마나 정확한지 정확도를 출력합니다.
print("정확도(Accuracy): %f" % compute_accuracy(ANN_model(x_test), y_test)) # 정확도: 약 94%
```

---

## **오토인코더(AutoEncoder)**

- 대표적인 비지도학습을 위한 인공신경망 구조 중 하나
    - 데이터의 숨겨진 구조를 발견하는 것이 목표
- 입력층의 노드 개수와 출력층의 노드 개수 같음, 은닉층
- 원본 데이터 재구축 → 핵심은 **은닉층의 출력값**
- 은닉층의 노드 개수는 입력, 출력층의 노드 개수보다 작음 ⇒ 더 작은 표현력으로 원본 데이터의 모든 특징들을 학습해야함 ⇒ **원본 데이터에서 불필요한 특징들을 제거한 압축된 특징**들을 학습하게 됨
- 압축된 특징을 나타내는 은닉층의 출력값을 원본 데이터 대신에 분류기의 입력으로 사용한다면 더 좋은 분류 성능 기대

**TensorFlow 2.0과 오토인코더를 이용한 MNIST 데이터 재구축**

```java
# -*- coding: utf-8 -*-
# AutoEncoder를 이용한 MNIST Reconstruction - Keras API를 이용한 구현

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# MNIST 데이터를 다운로드 합니다.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
# 이미지들을 float32 데이터 타입으로 변경합니다.
x_train, x_test = x_train.astype('float32'), x_test.astype('float32')
# 28*28 형태의 이미지를 784차원으로 flattening 합니다.
x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])
# [0, 255] 사이의 값을 [0, 1]사이의 값으로 Normalize합니다.
x_train, x_test = x_train / 255., x_test / 255.

# 학습에 필요한 설정값들을 정의합니다.
learning_rate = 0.02
training_epochs = 50    # 반복횟수
batch_size = 256        # 배치개수
display_step = 1        # 손실함수 출력 주기
examples_to_show = 10   # 보여줄 MNIST Reconstruction 이미지 개수
input_size = 784        # 28*28
hidden1_size = 256 
hidden2_size = 128

# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.
# 비지도 학습이므로 y data 필요 없음
train_data = tf.data.Dataset.from_tensor_slices(x_train)
train_data = train_data.shuffle(60000).batch(batch_size)

def random_normal_intializer_with_stddev_1():
  return tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None)

# tf.keras.Model을 이용해서 Autoencoder 모델을 정의합니다.
class AutoEncoder(tf.keras.Model):
  def __init__(self):
    super(AutoEncoder, self).__init__()
    # 인코딩(Encoding) - 784 -> 256 -> 128
    self.hidden_layer_1 = tf.keras.layers.Dense(hidden1_size,
                                                activation='sigmoid',
                                                kernel_initializer=random_normal_intializer_with_stddev_1(),
                                                bias_initializer=random_normal_intializer_with_stddev_1())
    self.hidden_layer_2 = tf.keras.layers.Dense(hidden2_size,
                                                activation='sigmoid',
                                                kernel_initializer=random_normal_intializer_with_stddev_1(),
                                                bias_initializer=random_normal_intializer_with_stddev_1())
    # 디코딩(Decoding) 128 -> 256 -> 784
    self.hidden_layer_3 = tf.keras.layers.Dense(hidden1_size,
                                                activation='sigmoid',
                                                kernel_initializer=random_normal_intializer_with_stddev_1(),
                                                bias_initializer=random_normal_intializer_with_stddev_1())
    self.output_layer = tf.keras.layers.Dense(input_size,
                                                activation='sigmoid',
                                                kernel_initializer=random_normal_intializer_with_stddev_1(),
                                                bias_initializer=random_normal_intializer_with_stddev_1())

  def call(self, x):
    H1_output = self.hidden_layer_1(x)
    H2_output = self.hidden_layer_2(H1_output)
    H3_output = self.hidden_layer_3(H2_output)
    reconstructed_x = self.output_layer(H3_output)

    return reconstructed_x

# MSE 손실 함수를 정의합니다.
@tf.function
def mse_loss(y_pred, y_true): #y_true = x
  return tf.reduce_mean(tf.pow(y_true - y_pred, 2)) # MSE(Mean of Squared Error) 손실함수

# 최적화를 위한 RMSProp 옵티마이저를 정의합니다.
optimizer = tf.optimizers.RMSprop(learning_rate)

# 최적화를 위한 function을 정의합니다.
@tf.function
def train_step(model, x):
  # 타겟데이터는 인풋데이터와 같습니다.
  y_true = x # 정답은 input x
  with tf.GradientTape() as tape:
    y_pred = model(x)
    loss = mse_loss(y_pred, y_true)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

# Autoencoder 모델을 선언합니다.
AutoEncoder_model = AutoEncoder()

# 지정된 횟수만큼 최적화를 수행합니다.
for epoch in range(training_epochs):
  # 모든 배치들에 대해서 최적화를 수행합니다.
  # Autoencoder는 Unsupervised Learning이므로 타겟 레이블(label) y가 필요하지 않습니다.
  for batch_x in train_data:
    # 옵티마이저를 실행해서 파라마터들을 업데이트합니다.
    _, current_loss = train_step(AutoEncoder_model, batch_x), mse_loss(AutoEncoder_model(batch_x), batch_x)
  # 지정된 epoch마다 학습결과를 출력합니다.
  if epoch % display_step == 0:
    print("반복(Epoch): %d, 손실 함수(Loss): %f" % ((epoch+1), current_loss))

# 테스트 데이터로 Reconstruction을 수행합니다.
reconstructed_result = AutoEncoder_model(x_test[:examples_to_show])
# 원본 MNIST 데이터와 Reconstruction 결과를 비교합니다.
f, a = plt.subplots(2, 10, figsize=(10, 2))
for i in range(examples_to_show):
  a[0][i].imshow(np.reshape(x_test[i], (28, 28)))
  a[1][i].imshow(np.reshape(reconstructed_result[i], (28, 28)))
f.savefig('reconstructed_mnist_image.png')  # reconstruction 결과를 png로 저장합니다.
f.show()
plt.draw()
plt.waitforbuttonpress()
```
